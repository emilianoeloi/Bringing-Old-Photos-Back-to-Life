# Restaura√ß√£o de foto antiga (implementa√ß√£o oficial do PyTorch)

<img src='imgs/0001.jpg'/>

### P√°gina do projeto | Papel (vers√£o CVPR) | Artigo (vers√£o do jornal) | Modelo Pr√©-treinado | Colab Demo üî•

**Trazendo fotos antigas de volta √† vida, CVPR2020 (oral)**

**Restaura√ß√£o de fotos antigas por meio da tradu√ß√£o profunda do espa√ßo latente, PAMI em revis√£o**

Ziyu Wan1, Bo Zhang2, Dongdong Chen3, Pan Zhang4, Dong Chen2, Jing Liao1, Fang Wen2
1 Universidade da cidade de Hong Kong, 2Microsoft Research √Åsia, 3Microsoft Cloud AI, 4USTC

## Novo

Agora voc√™ pode brincar com nosso Colab e test√°-lo em suas fotos.

## Requerimento

O c√≥digo √© testado no Ubuntu com GPUs Nvidia e CUDA instalados. Python> = 3.6 √© necess√°rio para executar o c√≥digo.

## Instala√ß√£o

Clone o reposit√≥rio Synchronized-BatchNorm-PyTorch para

```
cd Face_Enhancement / models / networks /
git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch
cp -rf Synchronized-BatchNorm-PyTorch / sync_batchnorm.
cd ../../../
```

```
cd Global / detec√ß√£o_models
git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch
cp -rf Synchronized-BatchNorm-PyTorch / sync_batchnorm.
cd ../../
```

Baixe o modelo pr√©-treinado de detec√ß√£o de pontos de refer√™ncia

```
cd Face_Detection /
wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
bzip2 -d shape_predictor_68_face_landmarks.dat.bz2
cd ../
```

Baixe o modelo pr√©-treinado do Armazenamento de Blob do Azure, coloque o arquivo Face_Enhancement / checkpoints.zip em ./Face_Enhancement e coloque o arquivo Global / checkpoints.zip em ./Global. Em seguida, descompacte-os respectivamente.

```
cd Face_Enhancement /
wget https://facevc.blob.core.windows.net/zhanbo/old_photo/pretrain/Face_Enhancement/checkpoints.zip
descompacte checkpoints.zip
cd ../
cd Global /
wget https://facevc.blob.core.windows.net/zhanbo/old_photo/pretrain/Global/checkpoints.zip
descompacte checkpoints.zip
cd ../
```

Instale depend√™ncias:

```bash
pip install -r requisitos.txt
```

## Como usar?

**Nota: GPU pode ser definido como 0 ou 0,1,2 ou 0,2; use -1 para CPU

Como usar?

Nota: GPU pode ser definido como 0 ou 0,1,2 ou 0,2; use -1 para CPU

### 1) Pipeline completo

Voc√™ pode restaurar facilmente as fotos antigas com um simples comando ap√≥s a instala√ß√£o e baixar o modelo pr√©-treinado.

Para imagens sem riscos:

```bash
python run.py --input_folder [test_image_folder_path] \
              --output_folder [output_path] \
              - GPU 0
```

Para imagens arranhadas:

```bash
python run.py --input_folder [test_image_folder_path] \
              --output_folder [output_path] \
              - GPU 0 \
              --with_scratch
```

Nota: Por favor, tente usar o caminho absoluto. Os resultados finais ser√£o salvos em ./output_path/final_output/. Voc√™ tamb√©m pode verificar os resultados produzidos de diferentes etapas em output_path.

### 2) Detec√ß√£o de arranh√µes

Atualmente, n√£o planejamos lan√ßar o conjunto de dados de fotos antigas arranhadas com r√≥tulos diretamente. Se voc√™ deseja obter os dados emparelhados, pode usar nosso modelo pr√©-treinado para testar as imagens coletadas e obter os r√≥tulos.

```bash
cd Global /
detec√ß√£o de python.py --test_path [test_image_folder_path] \
                    --output_dir [output_path] \
                    --input_size [resize_256 | full_size | scale_256]
```
<img src='imgs/scratch_detection.png'>

### 3) Restaura√ß√£o Global

Uma rede de tradu√ß√£o de dom√≠nio tripleto √© proposta para resolver a degrada√ß√£o estruturada e a degrada√ß√£o n√£o estruturada de fotos antigas.

<p align="center">
<img src='imgs/pipeline.PNG' width="50%" height="50%"/>
</p>


```bash
cd Global /
python test.py --Scratch_and_Quality_restore \
               --test_input [test_image_folder_path] \
               --test_mask [m√°scara correspondente] \
               --outputs_dir [output_path]

python test.py --Quality_restore \
 --test_input [test_image_folder_path] \
 --outputs_dir [output_path]
```

<img src='imgs/global.png'>

### 4) Melhoramento facial

Usamos um gerador progressivo para refinar as regi√µes do rosto de fotos antigas. Mais detalhes podem ser encontrados em nossa submiss√£o de peri√≥dicos e na pasta ./Face_Enhancement.

<p align="center">
<img src='imgs/face_pipeline.jpg' width="60%" height="60%"/>
</p>

<img src='imgs/face.png'>


> NOTA: Este repo √© principalmente para fins de pesquisa e ainda n√£o otimizamos o desempenho de >execu√ß√£o.

> Como o modelo √© pr√©-treinado com imagens de 256 * 256, o modelo pode n√£o funcionar idealmente para resolu√ß√£o arbitr√°ria.

## Fa√ßam

 Limpe o c√≥digo de teste
 Liberar modelo pr√©-treinado
 Demonstra√ß√£o Collab
 Substitua o m√≥dulo de detec√ß√£o de rosto (dlib) por RetinaFace
 Liberar c√≥digo de treinamento

## Cita√ß√£o

Se voc√™ acha nosso trabalho √∫til para sua pesquisa, considere citar os seguintes artigos :)

```
@inproceedings {wan2020bringing,
title = {Trazendo fotos antigas de volta √† vida},
autor = {Wan, Ziyu e Zhang, Bo e Chen, Dongdong e Zhang, Pan e Chen, Dong e Liao, Jing e Wen, Fang},
booktitle = {Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition},
p√°ginas = {2747--2757},
ano = {2020}
}
```

```
@misc {2009.07047,
Autor = {Ziyu Wan e Bo Zhang e Dongdong Chen e Pan Zhang e Dong Chen e Jing Liao e Fang Wen},
Title = {Old Photo Restoration via Deep Latent Space Translation},
Ano = {2020},
Eprint = {arXiv: 2009.07047},
}
```

Se voc√™ tamb√©m estiver interessado na coloriza√ß√£o de fotos / v√≠deos legados, consulte este trabalho.

## Manuten√ß√£o

Este projeto √© mantido atualmente por Ziyu Wan e √© apenas para uso em pesquisa acad√™mica. Se voc√™ tiver alguma d√∫vida, sinta-se √† vontade para entrar em contato com raywzy@gmail.com.

## Licen√ßa

Os c√≥digos e o modelo pr√©-treinado neste reposit√≥rio est√£o sob a licen√ßa MIT conforme especificado no arquivo LICENSE. Usamos nosso conjunto de dados rotulado para treinar o modelo de detec√ß√£o de arranh√µes.

Este projeto adotou o C√≥digo de Conduta Open Source da Microsoft. Para obter mais informa√ß√µes, consulte as Perguntas frequentes do C√≥digo de Conduta ou entre em contato com opencode@microsoft.com com perguntas ou coment√°rios adicionais.